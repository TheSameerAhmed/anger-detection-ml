{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all files with low validation scores from the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all rows in the dataset with low agreement between annotators, this is determined to be labels with a difference > |2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_ozxaRrIh9Z"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GHuvxosIa4X",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install numpy\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install sklearns\n",
    "# !{sys.executable} -m pip install matplot\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "file_path='CleanData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dfs(train, val):\n",
    "  '''\n",
    "    Join the validation and training datasets in order to compare the ratings\n",
    "  '''\n",
    "  train['file_name'] = train['file_name'].str.strip()\n",
    "  val['file_name'] = val['file_name'].str.strip()\n",
    "\n",
    "  df_merged = pd.merge(train,val,on='file_name',how='left',suffixes=('_train', '_val'))\n",
    "\n",
    "  # IOE on segment label\n",
    "  df_merged['match_s_diff'] = df_merged['segment_label_train'] - df_merged['segment_label_val'] \n",
    "  df_merged['match_s_diff_abs'] = df_merged['match_s_diff'].abs()\n",
    "  \n",
    "  # IOE on parent label\n",
    "  df_merged['match_p_diff'] = df_merged['parent_label_train'] - df_merged['parent_label_val'] \n",
    "  df_merged['match_p_diff_abs'] = df_merged['match_p_diff'].abs()\n",
    "  return(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files(df, dataset_name):\n",
    "  '''\n",
    "    Remove audio clips that have a label difference of more then 2 levels\n",
    "  '''\n",
    "  \n",
    "  df_to_remove = df.drop(df[(df.match_s_diff_abs < 2) & (df.match_p_diff_abs < 2)].index)\n",
    "  \n",
    "  df_to_remove =  df_to_remove[df_to_remove['segment_label_val'].notna()]\n",
    "  \n",
    "  df_to_remove = df_to_remove[['file_name', 'parent_file_train']]\n",
    "  \n",
    "  df_to_remove = df_to_remove.reset_index()\n",
    "\n",
    "  removal_dict ={}\n",
    "  \n",
    "  for index, row in df_to_remove.iterrows():\n",
    "    if row['parent_file_train'] not in removal_dict.keys():\n",
    "      removal_dict[row['parent_file_train']] = [row['file_name']]\n",
    "    else:\n",
    "      removal_dict[row['parent_file_train']].append(row['file_name'])\n",
    "  \n",
    "  for parent_file in removal_dict:\n",
    "    for file_name in removal_dict[parent_file]:\n",
    "      try:\n",
    "        #remove file with poor validation\n",
    "        os.remove(f\"{dataset_name}/{parent_file}/{file_name}.wav\")\n",
    "      except FileNotFoundError as e:\n",
    "        print(e)\n",
    "\n",
    "  print(\"All low validation files removed\")\n",
    "\n",
    "  pathlist = Path(dataset_name).glob('**/*_processed.wav')\n",
    "  for path in pathlist:\n",
    "       # because path is object not string\n",
    "       path_in_str = str(path)\n",
    "       os.remove(path_in_str)\n",
    "  \n",
    "  pathlist = Path(dataset_name).glob('**/*_raw.wav')\n",
    "  for path in pathlist:\n",
    "       # because path is object not string\n",
    "       path_in_str = str(path)\n",
    "       os.remove(path_in_str)\n",
    "  \n",
    "  \n",
    "  pathlist = Path(dataset_name).glob('**/*_raw.aac')\n",
    "  for path in pathlist:\n",
    "       # because path is object not string\n",
    "       path_in_str = str(path)\n",
    "       os.remove(path_in_str)\n",
    "\n",
    "  print(\"All extra files removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows(df, filename, type):\n",
    "  '''\n",
    "    Remove the rows from the training label csv to reflect removed audio clips\n",
    "    type is either 'train' or 'validate' depending which of the two files you are cleaning\n",
    "  '''\n",
    "\n",
    "  #the set you want to generate is he opposite yof the one you will drop\n",
    "  if type == 'validate':\n",
    "    set = 'train'\n",
    "  elif  type == 'train':\n",
    "    set = 'val'\n",
    "\n",
    "  df_to_keep = df.drop(df[(df.match_s_diff_abs >= 2) | (df.match_p_diff_abs >= 2)].index)\n",
    "\n",
    "  df_to_keep =  df_to_keep[df_to_keep['match_s_diff_abs'].notna()]\n",
    "\n",
    "  df_to_keep = df_to_keep[df_to_keep.columns.drop(list(df_to_keep.filter(regex=f'_{set}')))]\n",
    "\n",
    "  df_to_keep = df_to_keep[df_to_keep.columns.drop(list(df_to_keep.filter(regex='match_')))]\n",
    "\n",
    "  df_to_keep.to_csv(f\"CleanData/ValDrop/{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part A` of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_train = pd.read_csv(file_path+'labelled-reddit-2021-08-12-A-PK.csv')\n",
    "anno_val = pd.read_csv(file_path+'labelled-reddit-2021-08-12-A-CW.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = join_dfs(anno_train, anno_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove files with low validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_files(df_merged, \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove low validation rows from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows(df_merged, \"A-cleaned.csv\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows(df_merged, \"A-cleaned-val.csv\", \"validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part E` of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_train = pd.read_csv(file_path+'labelled-reddit-2021-08-12-E-VP.csv')\n",
    "anno_val = pd.read_csv(file_path+'labelled-reddit-2021-08-12-E-PK.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = join_dfs(anno_train, anno_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove files with low validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_files(df_merged, \"E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove files with low validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows(df_merged, \"E-cleaned.csv\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows(df_merged, \"E-cleaned-val.csv\", \"validate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_train = pd.read_csv('./CleanData/ValDrop/E-cleaned.csv')\n",
    "features = pd.read_csv('./extracted/E-features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the files were not labeled at all, remove these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_features = features[features['filepath'].isin(anno_train['file_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_features.to_csv(f\"extracted/E-features-clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part H` of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_train = pd.read_csv(file_path+'labelled-reddit-2021-08-31-H-KK_SA.csv')\n",
    "anno_val = pd.read_csv(file_path+'labelled-reddit-2021-08-31-H-TL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = join_dfs(anno_train, anno_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove files with low validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_files(df_merged, \"H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove files with low validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows(df_merged, \"H-cleaned.csv\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows(df_merged, \"H-cleaned-val.csv\", \"validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the files were not labeled at all, remove these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_train = pd.read_csv('./CleanData/ValDrop/H-cleaned.csv')\n",
    "features = pd.read_csv('./extracted/H-features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_features = features[features['filepath'].isin(anno_train['file_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_features.to_csv(f\"extracted/H-features-clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Part F` of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_train = pd.read_csv(file_path+'labelled-reddit-2021-08-12-F-SA.csv')\n",
    "anno_val = pd.read_csv(file_path+'labelled-reddit-2021-08-12-F-CW.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = join_dfs(anno_train, anno_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove files with low validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_files(df_merged, \"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove files with low validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows(df_merged, \"F-cleaned.csv\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rows(df_merged, \"F-cleaned-val.csv\", \"validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the files were not labeled at all, remove these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_train = pd.read_csv('./CleanData/ValDrop/F-cleaned.csv')\n",
    "features = pd.read_csv('./extracted/F-features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_features = features[features['filepath'].isin(anno_train['file_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_features.to_csv(f\"extracted/F-features-clean.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76138169b66bb331c35e896ea62a05619ac4c0d9b78a4f76b6fa7685e9ed89d1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
